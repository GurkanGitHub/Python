{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff831de",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue\" align=\"center\">Handwritten digits classification using neural network</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e16c8e9",
   "metadata": {},
   "source": [
    "In this notebook we will classify handwritten digits using a simple neural network which has only input and output layers. We will than add a hidden layer and see how the performance of the model improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83299bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3703098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847145e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e41e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee48ad12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ccd1d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9e89042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db9b5884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]#this image is represented in 2-dim array like this, zoros are black points. 255 are white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75822f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20cdee5d090>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ac39f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5830525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dfebb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to flatten our data set. convert 28*28 image into the single dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deacf1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape # X_train has 2 dim. 1st is the number of samples, 2nd and 3th dim is each ind. image\n",
    "              # we will turn it into (60000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a72abe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255 # later added. since low accuracy. scaling need to get high acuracy.\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17d6c60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
       "        0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117647,\n",
       "        0.36862745, 0.60392157, 0.66666667, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235294, 0.6745098 ,\n",
       "        0.99215686, 0.94901961, 0.76470588, 0.25098039, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215686, 0.93333333, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.98431373, 0.36470588, 0.32156863,\n",
       "        0.32156863, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882353, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.77647059,\n",
       "        0.71372549, 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31372549, 0.61176471,\n",
       "        0.41960784, 0.99215686, 0.99215686, 0.80392157, 0.04313725,\n",
       "        0.        , 0.16862745, 0.60392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509804, 0.99215686, 0.74509804, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313725, 0.74509804, 0.99215686, 0.2745098 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.1372549 , 0.94509804, 0.88235294,\n",
       "        0.62745098, 0.42352941, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764706, 0.94117647,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "        0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.36470588, 0.98823529, 0.99215686, 0.73333333,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.97647059, 0.99215686, 0.97647059,\n",
       "        0.25098039, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980392, 0.71764706, 0.99215686, 0.99215686, 0.81176471,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.58039216, 0.89803922,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.71372549,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882353,\n",
       "        0.83529412, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.77647059, 0.31764706, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058824, 0.85882353, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.76470588, 0.31372549,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568627,\n",
       "        0.6745098 , 0.88627451, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156863, 0.04313725, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333333,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137255, 0.52941176,\n",
       "        0.51764706, 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e3d4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you do not bother yourself with this flattened layer, use Keras spatial layer called flattened."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e3ee6",
   "metadata": {},
   "source": [
    "<h3 style='color:purple'>Very simple neural network with no hidden layers</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff64e8",
   "metadata": {},
   "source": [
    "<img src=\"digits_nn.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d797db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2735 - accuracy: 0.9220\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1256 - accuracy: 0.9626\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0885 - accuracy: 0.9741\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0674 - accuracy: 0.9798\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0531 - accuracy: 0.9838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x20ce222aa90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([ # sequential means i am having a stack of layers in my nn.\n",
    "    keras.layers.Flatten(input_shape = (28,28)), # flatten flattens the 28-28 vector into 786*1 to input to first layer\n",
    "    keras.layers.Dense(100, activation = \"relu\"),\n",
    "    keras.layers.Dense(10,  activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir = \"logs/\", histogram_freq=1)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = \"adam\", # try with SGD-stochastic grad descent\n",
    "    loss =\"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 5, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3628743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 9s 4ms/step - loss: 0.6445 - accuracy: 0.8374\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3378 - accuracy: 0.9055\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2905 - accuracy: 0.9169\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2609 - accuracy: 0.9259\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2385 - accuracy: 0.9327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x20c8dc43690>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([ # sequential means i am having a stack of layers in my nn.\n",
    "    keras.layers.Flatten(input_shape = (28,28)), # flatten flattens the 28-28 vector into 786*1 to input to first layer\n",
    "    keras.layers.Dense(100, activation = \"relu\"),\n",
    "    keras.layers.Dense(10,  activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir = \"logs/SGD\", histogram_freq=1)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = \"SGD\", # try with SGD-stochastic grad descent\n",
    "    loss =\"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 5, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a54b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard run this code on git bash: tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90080896",
   "metadata": {},
   "source": [
    "#launching tensorboard in notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c89b5",
   "metadata": {},
   "source": [
    "## Manual Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b4cfc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train.reshape(len(X_train),28*28)\n",
    "X_test_flattened = X_test.reshape(len(X_test),28*28)\n",
    "print(X_train_flattened.shape)\n",
    "print(X_test_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7deb0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating a simple nn- one input and one output layer.\n",
    "\n",
    "model = keras.Sequential([ \n",
    "    keras.layers.Dense(10, input_shape= (784,), activation = \"sigmoid\") # since this is a stack, it will accept every layer as one element, output and input layer\n",
    "                                                                        #10 is output neuron\n",
    "])\n",
    "\n",
    "\n",
    "#compiling nn, derlemek-toplamak\n",
    "model.compile(\n",
    "    optimizer = \"adam\", # allows to train efficiently.when backward propagation and training is going on optimizer will allow you to reach global optima in efficient way.\n",
    "    loss =\"sparse_categorical_crossentropy\", # output class is categorical. 0-9. sparse means, y_train-output variable is integer number.\n",
    "                                             # if it is 1 hot encoded array you would use categorical loss. MSE and MAE should also be used.\n",
    "    metrics = [\"accuracy\"])\n",
    "\n",
    "#training\n",
    "\n",
    "model.fit(X_train_flattened, y_train, epochs = 5) #epoch: num of iteration for which nn will run the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "88c93069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each epoch: it go through 1875 samples and at the end of the run it measures loss and accuracy.\n",
    "# notice that as time goes  accuracy increases. 1 is perfect. If it is low, scale the values\n",
    "#divide the whole array by 255, you get values between 0-1\n",
    "# 02 percent a time it will make accurate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b97a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the accuracy on the test data set\n",
    "model.evaluate(X_test_flattened, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4764d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "model.predict(X_test_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a18e35af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb27e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_test_flattened)\n",
    "y_predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "decbaf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "265883d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_labels = [np.argmax(i) for i in y_predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "74cd5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = tf.math.confusion_matrix(labels = y_test, predictions = y_predicted_labels)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f09ae2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot = True, fmt = \"d\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "939cb396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lots of errors, add hidden layer: improves the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c7d683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, input_shape= (784,), activation = \"relu\"), # HIDDEN LAYER, how many neuron wanted: try&error, no rule of thumb, just start with a value, less than input shape\n",
    "    keras.layers.Dense(10, activation = \"sigmoid\") #last layer need not input shape\n",
    "                                                                        #because whatever 1st layer it is connected, it knows how to figure out the input shape.\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss =\"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"])\n",
    "\n",
    "model.fit(X_train_flattened, y_train, epochs = 5) #epoch: num of iteration for which nn will run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8c41e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_flattened, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb5d6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_test_flattened)\n",
    "y_predicted_labels = [np.argmax(i) for i in y_predicted]\n",
    "cm = tf.math.confusion_matrix(labels = y_test, predictions = y_predicted_labels)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot = True, fmt = \"d\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Truth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc9efbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
